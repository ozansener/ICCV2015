\section{Introduction}
Leaning the instructions of a novel non-trivial task is both a challange and a necessity for both humans and autonomous systems. This necessity resulted in many community generated instruction collections \cite{wikiHow,eHow} and expert curated recipe books\cite{recipeBook1,recipeBook2}. However, this instructions are generally based on a language modality and explains a single way of performing the task although there are variety of ways. On the other hand, online video storage services are full of unstructured instructional videos\footnote{YouTube has 281.000 videos for \emph{"How to tie a bow tie"}} covering variety of ways, environment conditions and view angles. Although there have been many succesfull attempts in detecting activities from videos \cite{act1,act2}, structural representation of such a large and useful video collection is not possible. In this paper, we focus on joint semantic representation of YouTube videos as a response to a single query. We specficially study the unsupervised joint-detection of the activities from a collection of YouTube videos.

Understanding of the instructional videos, requires the careful processing of two complementary modaliities namely language and the vision.  Luckily the target domain, YouTube videos, has unstructured subtitles as well. They are either generated by the content developer (5\% of the time) or automatically generated by using the Automatic Speech Recognition (ASR) softwares. The main limitations of the existing activity detection literature for this problem is scalability and representation level. Existing approaches are mainly supervised and requires extensive training set which is not tractable in the scale of YouTube videos. Moreover, current activity detection research focuses on the low-level visual features. However, such videos in the wild have objects with completely different texture and shape characteristics from wide range of views. Instead, we focus on extracting high-level visual semantic representations and using salient words occuring among the videos.

We rely on the assumption that the videos collected as the response of a same instructional query, share similar activities performed by the similar objects. We start with the independent processing of the videos in order to create a large collection of visual object proposals and words. After the proposal generation, we jointly process the proposal collections and words to detect the visual objects and words which can be used to represent the unstructured information. Since we rely on high-level information instead of the low-level features, the resulting objects represent the semantic information instead of visual characteristic. By using the extracted objects, we compute the holistic representation of the multi-modal information in each frame.

Moving from frame-wise visual understading to activity understanding, requires the joint processing of all the videos with the temporal information. In order to exploit the temporal information, we model each video as a Hidden Markov Model using state space of activities. Since we assume that the videos share some of the activities and we have no supervision, we use a model based on \emph{beta process mixture model}. Our model jointly learn the activities and detect them in the videos. Moreover, it does not require prior knowledge over the number of activities.

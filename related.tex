% !TEX root = recipeUnderstanding.tex

\section{Related Work}
% \vspace{-1mm}

Three aspects differentiate this work from the majority of existing techniques: 1) discovering semantic steps from a video category, 2) being unsupervised, 3) adopting a multi-modal joint vision-language model for video parsing. A thorough review of the related literature is provided below.

\noindent\textbf{Video Summarization:} Summarizing an input video as a sequence of key frames (static) or video clips (dynamic) is useful for both multimedia search interfaces and retrieval purposes. Early works in the area are summarized in~\cite{vidAbstraction} and mostly focus on \emph{choosing keyframes} for visualization.
%Keyframes are also improved by using the video tags and spatio-temporal information~\cite{beyondSearch,createSum}.
%% AMIR: put a '~' instead of ' ' before \cite

%%AMIR: could be briefed
Summarizing videos is particularly important for some specific domains like ego-centric videos and news reports as they are generally long in duration. There are many succesful works~\cite{lee2012discovering, lu2013story,rui2000automatically}; however, they mostly rely on characteristics specific to the domain.

%http://www.sangminoh.org/Publications_files/Oh2012bmvc_videography.pdf --> learn atomic representations and do not do a segmentation
Summarization is also applied to the large image collections by recovering the temporal ordering and visual similarity of images \cite{storyGraph}, and by Gupta et al.~\cite{gupta2009understanding} to videos in a supervised framework using annotations of actions. These collections are also used to choose important view points for key-frame selection \cite{khosla2013large} and further extended to video clip selection \cite{kim2014joint,potapov2014category}. Unlike all of these methods which  focus on forming a set of key frames/clips for a compact summary (which is not necessarily semantically meaningful), we provide a fresh approach to video summarization by performing it through semantic parsing on vision and language. However, regardless of this dissimilarity, we experimentally compare our method against them.

\noindent\textbf{Modeling Visual and Language Information:}
Learning the relationship between the visual and language data is a crucial problem due to its immense applications. Early methods \cite{matching} in this area focus on learning a common multi-modal space in order to jointly represent language and vision. They are further extended to learning higher level relations between object segments and words \cite{connecting}. Similarly, 
Zitnick et al.~\cite{zitnick2013learning,zitnick2013bringing} used abstracted clip-arts to understand spatial relations of objects and their language correspondences. Kong et al.~\cite{kong2014you} and 
Fidler et al.~\cite{fidler2013sentence} both accomplished the task of learning spatial reasoning by only using the image captions. Relations extracted from image-caption pairs, are further used to help semantic parsing \cite{yu2013grounded} and activity recognition \cite{motwani2012improving}. Recent works also focus on automatic generation of image captions with underlying ideas ranging from finding similar images and transferring their captions \cite{ordonez2011im2text} to learning language models conditioned on the image features \cite{kiros2014multimodal,socher2014grounded,farhadi2010every}; their employed approach to learning language models is typically either based on graphical models \cite{farhadi2010every} or neural networks \cite{socher2014grounded,kiros2014multimodal,deepAlignment}.

All aforementioned methods are using supervised labels either as strong image-word pairs or weak image-caption pairs, while our method is fully unsupervised.

\noindent\textbf{Activity/Event Recognition:}
The literature of activity recognition is broad. The closest techniques to ours are either supervised or focus on detecting a particular (and often short) action in a weakly/unsupervised manner. Also, a large body of action recognition methods are intended for trimmed videos clips or remain limited to detecting very short actions~\cite{kuehne2011hmdb, UCF101, niebles10_eccv, laptev08_cvpr, efros03_iccv, ryoo09_iccv}. Even though some recent works attempted action recognition in untrimmed videos~\cite{THUMOS14, oneata2014lear, jainuniversity}, they are mostly fully supervised.

Additionally, several method for localizing instances of actions in rather longer video sequences have been developed~\cite{duchenne09_iccv, hoai11_cvpr, laptev07_iccv, bojanowski14_eccv, pirsiavash14_cvpr}. Our work is different from those in terms of being multimodal, unsupervised, applicable to a video collection, and not limited to identifying predefined actions or the ones with short temporal spans.
%Representing actions as 2D+t tubes is a common strategy for action recognition~\cite{blank05_iccv, brendel11_iccv, ma13_iccv}. Recently, there are works that use hierarchical spatiotemporal segments to capture the multi-scale characteristics of actions \cite{brendel11_iccv, ma13_iccv}. Our representation differs in that we can discover the action-related spatiotemporal segments by joint processing the video collection from a pool of proposals.
Also, the previous works on finding action primitives such as~\cite{niebles10_eccv, yao10b_cvpr, jain13_cvpr,lan14_eccv, lan14_vs} are primarily limited to discovering atomic sub-actions, and therefore, fail to identify complex and high-level parts of a long video.

Recently, event recounting has attracted much interest and intends to identify the evidential segments for which a video belongs to a certain class~\cite{sun2014discover,das2013thousand,barbu2012video}. Event recounting is a relatively new topic and the existing methods mostly employ a supervised approach. Also, their end goal is to identify what parts of a video are highly related to an event, and not parsing the video into semantic steps.
%This setup is similar to Hughes et al.\cite{npActivity}. However, we differ in the choices of the underlying distributions since we based our model on semantic multi-modal information.

% AMIR: flow graph sounds like 'storyline'. Consider replacing/removing it.
%Mori et al.\cite{flowGraph} also learns the relations of the actions in terms of a flow graph with the help of a supervision.

\noindent\textbf{Recipe Understanding:}
Following the interest in community generated recipes in the web, there have been many attempts to automatically process recipes. Recent methods on natural language processing \cite{cookingSemantics,logicRecipe} focus on semantic parsing of language recipes in order to extract actions and the objects in the form of predicates. Tenorth et al.~\cite{logicRecipe} further process the predicates in order to form a complete logic plan. The aforementioned approaches focus only on the language modality and they are not applicable to the videos. The recent advances \cite{beetz,cookie} in robotics use the parsed recipe in order to perform cooking tasks. They use supervised object detectors and report a successful autonomous experiment. In addition to the language based approaches, Malmaud et al.~\cite{alignment} consider both language and vision modalities and propose a method to align an input video to a recipe. However, it can not extract the steps automatically and requires a ground truth recipe to align. On the contrary, our method uses both visual and language modalities and extracts the actions while autonomously discovering the steps. There is also an approach which generates multi-modal recipes from expert demonstrations \cite{photoshop}. However, it is developed only for the domain of ``teaching user interfaces" and are not applicable to videos.

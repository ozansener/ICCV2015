\section{Related Work}
\paragraph{Video Summarization}
Summarizing an input video as a sequence of keyframes (static) or as a sequence of video clips (dynamic) is useful for both multi-media search interfaces and retrieval purposes. Early works in the area are summarized in \cite{vidAbstraction} and mostly focus on choosing key-frames for visualization. Idea of choosing key-frames is also extended by using the video tags by Hong et al\cite{beyondSearch} and using the spatio-temporal information by Gygli et al.\cite{createSum}.

Summarizing videos is crucial for ego-centric videos since the ego-centric videos are generally long in duration. There are many works which succesfully segment such videos in to a sequence of important shots \cite{lee2012discovering, lu2013story}; however, they mostly rely on specific features of ege-centric videos. Rui et al. \cite{rui2000automatically} proposed another dynamic summarization method based on the excitement in the speech of the reporter. Due to their domain specific designs, these algorithms are not applicable to the general instructional videos.

%http://www.sangminoh.org/Publications_files/Oh2012bmvc_videography.pdf --> learn atomic representations and do not do a segmentation
Same idea is also applied to the large image collecitons by recovering the temporal ordering and visual similarity of images \cite{storyGraph}. This image colelctions are further used to choose important view points for video key-frame selection by Khosla et al.\cite{khosla2013large}. And further extended to video clip selection by Kim et al\cite{kim2014joint} and Potapov et al.\cite{potapov2014category}. Although they are different from our approach since they do not use any high-level semantic information or the language information, we experimentally compare our method with them.

\paragraph{Understanding Multi-Modal Information:}
Learning the relationship between the visual and language data is a crucial problem due to its immense multimedia applications. Early methods \cite{matching} in the area focus on learning a common multi-modal space in order to jointly represent language and vision. They are also extended to learning higher level relations between object segments and words \cite{connecting}. Zitnick et al.\cite{zitnick2013learning,zitnick2013bringing} used abstracted clip-arts to further understand spatial relations of objects and their language correspondences. Kong et al. \cite{kong2014you} and Fidler et al. \cite{fidler2013sentence} both accomplished the same task by using the image captions only. Relations extracted from image-caption pairs, are further used to help semantic parsing \cite{yu2013grounded} and activity recognition \cite{motwani2012improving}. Recent works also focused on generating image captions automatically using the input image. These methods range from finding similar images and using their captions \cite{ordonez2011im2text} to learning language modal conditioned on the image \cite{kiros2014multimodal,socher2014grounded,farhadi2010every}. And the methods to learn language models vary from graphical models \cite{farhadi2010every} to neural networks \cite{socher2014grounded,kiros2014multimodal,deepAlignment}.

All afforementioned methods are using supervised labels either as strong image-word pairs or weak image-caption pairs. On the other hand, our method is fully unsupervised.

\paragraph{Activity Detection/Recognition:}

%This setup is similar to Hughes et al.\cite{npActivity}. However, we differ in the choices of the underlying distributions since we based our model on semantic multi-modal information.

\paragraph{Recipe Understanding}
Following the interest in community generated recipes in the web, there have been many attempts to automatically process recipes. Recent methods on natural language processing \cite{cookingSemantics,logicRecipe} focus on semantic parsing of language recipes in order to extract actions and the objects in the form of predicates. Tenorth et al.\cite{logicRecipe} further process the predicates in order to form a complete logic plan. Mori et al.\cite{flowGraph} also learns the relations of the actions in terms of a flow graph with the help of a supervision. The aforementioned approaches focus only on the language modality and they are not applicable to the videos. We have also seen recent advances \cite{beetz,cookie} in robotics which uses the parsed recipe in order to perform cooking tasks. They use supervised object detectors and report a succesfull autonomous cooking experiment. In addition to the language based approaches, Malmaud et al.\cite{alignment} consider both language and vision modalities and propose a method to align an input video to a recipe. However, it can not extract the steps/actions automatically and requires a ground truth recipe to align. On the contrary, our method uses both visual and language modalities and extracts the actions while autonomously constructing the recipe. There is also an approach which generates multi-modal recipes from expert demonstrations \cite{photoshop}. However, it is developed only for the domain of \emph{teaching user interfaces} and are not applicable to the videos.

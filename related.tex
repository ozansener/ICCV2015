\section{Related Work}
Three key aspects differentiate this work from the majority of existing techniques for similar tasks: 1) providing a semantic parsing of a video category leading to a compact storyline representation, 2) being unsupervised, 3) adopting a multi-modal joint vision-language model for parsing. A thorough review of the related literature is provided in this Section.

\paragraph{Video Summarization}
Summarizing an input video as a sequence of key frames (static) or as a sequence of video clips (dynamic) is useful for both multimedia search interfaces and retrieval purposes. Early works in the area are summarized in \cite{vidAbstraction} and mostly focus on choosing keyframes for visualization. Keyframes are also improved by using the video tags by Hong et al.\cite{beyondSearch} and using the spatio-temporal information by Gygli et al.\cite{createSum}.

Summarizing videos is particularly important for ego-centric videos as they are generally long in duration. There are many works which successfully segment such videos into a sequence of important shots \cite{lee2012discovering, lu2013story}; however, they mostly rely on the specific characteristics of edge-centric videos. Rui et al. \cite{rui2000automatically} proposed another dynamic summarization method based on the excitement of the speech of the reporter. Due to their domain specific designs, these algorithms are neither applicable to the general video collections nor to the instructional videos.

%http://www.sangminoh.org/Publications_files/Oh2012bmvc_videography.pdf --> learn atomic representations and do not do a segmentation
Summarization is also applied to the large image collections by recovering the temporal ordering and visual similarity of images \cite{storyGraph}. This image collections are further used to choose important view points for video key-frame selection by Khosla et al.\cite{khosla2013large}. And further extended to video clip selection by Kim et al.\cite{kim2014joint} and Potapov et al.\cite{potapov2014category}. We provide a fresh approach to video summarization by performing it through semantic parsing. Although existing summarization techniques are dissimilar to ours in terms of using high-level semantics or language information, we experimentally compare our method with them.

\paragraph{Understanding Multi-Modal Information:}
Learning the relationship between the visual and language data is a crucial problem due to its immense applications. Early methods \cite{matching} in the area focus on learning a common multi-modal space in order to jointly represent language and vision. They are further extended to learning higher level relations between object segments and words \cite{connecting}. Similarly, Zitnick et al.\cite{zitnick2013learning,zitnick2013bringing} used abstracted clip-arts to understand spatial relations of objects and their language correspondences. Kong et al. \cite{kong2014you} and Fidler et al. \cite{fidler2013sentence} both accomplished the task of learning spatial reasoning by only using the image captions. Relations extracted from image-caption pairs, are further used to help semantic parsing \cite{yu2013grounded} and activity recognition \cite{motwani2012improving}. Recent works also focused on generating image captions automatically using the input image. These methods range from finding similar images and using their captions \cite{ordonez2011im2text} to learning language modal conditioned on the image \cite{kiros2014multimodal,socher2014grounded,farhadi2010every}. And the methods to learn language models vary from graphical models \cite{farhadi2010every} to neural networks \cite{socher2014grounded,kiros2014multimodal,deepAlignment}.

All aforementioned methods are using supervised labels either as strong image-word pairs or weak image-caption pairs. On the other hand, our method is fully unsupervised.

\paragraph{Activity Detection/Recognition:}

%This setup is similar to Hughes et al.\cite{npActivity}. However, we differ in the choices of the underlying distributions since we based our model on semantic multi-modal information.

\paragraph{Recipe Understanding}
Following the interest in community generated recipes in the web, there have been many attempts to automatically process recipes. Recent methods on natural language processing \cite{cookingSemantics,logicRecipe} focus on semantic parsing of language recipes in order to extract actions and the objects in the form of predicates. Tenorth et al.\cite{logicRecipe} further process the predicates in order to form a complete logic plan. Mori et al.\cite{flowGraph} also learns the relations of the actions in terms of a flow graph with the help of a supervision. The aforementioned approaches focus only on the language modality and they are not applicable to the videos. The recent advances \cite{beetz,cookie} in robotics use the parsed recipe in order to perform cooking tasks. They use supervised object detectors and report a successful autonomous cooking experiment. In addition to the language based approaches, Malmaud et al.\cite{alignment} consider both language and vision modalities and propose a method to align an input video to a recipe. However, it can not extract the steps/actions automatically and requires a ground truth recipe to align. On the contrary, our method uses both visual and language modalities and extracts the actions while autonomously constructing the recipe. There is also an approach which generates multi-modal recipes from expert demonstrations \cite{photoshop}. However, it is developed only for the domain of \emph{teaching user interfaces} and are not applicable to the videos.

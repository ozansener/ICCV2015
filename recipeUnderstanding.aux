\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}\hskip -1em.\nobreakspace  {}Introduction}{1}{section.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Given a large video collection, we discover semantic activity steps without any supervision by using available multi-modal information (visual frames and subtitles). We also parse each video based on the discovered steps.\relax }}{1}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{teaser}{{1}{1}{Given a large video collection, we discover semantic activity steps without any supervision by using available multi-modal information (visual frames and subtitles). We also parse each video based on the discovered steps.\relax }{figure.caption.1}{}}
\citation{vidAbstraction}
\citation{lee2012discovering}
\citation{lu2013story}
\citation{rui2000automatically}
\citation{storyGraph}
\citation{gupta2009understanding}
\citation{khosla2013large}
\citation{kim2014joint}
\citation{potapov2014category}
\citation{matching}
\citation{connecting}
\citation{zitnick2013learning}
\citation{zitnick2013bringing}
\citation{kong2014you}
\citation{fidler2013sentence}
\citation{yu2013grounded}
\citation{motwani2012improving}
\citation{ordonez2011im2text}
\citation{kiros2014multimodal}
\citation{socher2014grounded}
\citation{farhadi2010every}
\citation{farhadi2010every}
\citation{socher2014grounded}
\citation{kiros2014multimodal}
\citation{deepAlignment}
\citation{kuehne2011hmdb}
\citation{UCF101}
\citation{niebles10_eccv}
\citation{laptev08_cvpr}
\citation{efros03_iccv}
\citation{ryoo09_iccv}
\citation{THUMOS14}
\citation{oneata2014lear}
\citation{jainuniversity}
\citation{duchenne09_iccv}
\citation{hoai11_cvpr}
\citation{laptev07_iccv}
\citation{bojanowski14_eccv}
\citation{pirsiavash14_cvpr}
\citation{niebles10_eccv}
\citation{yao10b_cvpr}
\citation{jain13_cvpr}
\citation{lan14_eccv}
\citation{lan14_vs}
\citation{sun2014discover}
\citation{das2013thousand}
\citation{barbu2012video}
\@writefile{toc}{\contentsline {section}{\numberline {2}\hskip -1em.\nobreakspace  {}Related Work}{2}{section.2}}
\@writefile{toc}{\contentsline {paragraph}{Video Summarization:}{2}{section*.2}}
\@writefile{brf}{\backcite{vidAbstraction}{{2}{2}{section*.2}}}
\@writefile{brf}{\backcite{lee2012discovering, lu2013story,rui2000automatically}{{2}{2}{section*.2}}}
\@writefile{brf}{\backcite{storyGraph}{{2}{2}{section*.2}}}
\@writefile{brf}{\backcite{gupta2009understanding}{{2}{2}{section*.2}}}
\@writefile{brf}{\backcite{khosla2013large}{{2}{2}{section*.2}}}
\@writefile{brf}{\backcite{kim2014joint,potapov2014category}{{2}{2}{section*.2}}}
\@writefile{toc}{\contentsline {paragraph}{Modeling Visual and Language Information:}{2}{section*.3}}
\@writefile{brf}{\backcite{matching}{{2}{2}{section*.3}}}
\@writefile{brf}{\backcite{connecting}{{2}{2}{section*.3}}}
\@writefile{brf}{\backcite{zitnick2013learning,zitnick2013bringing}{{2}{2}{section*.3}}}
\@writefile{brf}{\backcite{kong2014you}{{2}{2}{section*.3}}}
\@writefile{brf}{\backcite{fidler2013sentence}{{2}{2}{section*.3}}}
\@writefile{brf}{\backcite{yu2013grounded}{{2}{2}{section*.3}}}
\@writefile{brf}{\backcite{motwani2012improving}{{2}{2}{section*.3}}}
\@writefile{brf}{\backcite{ordonez2011im2text}{{2}{2}{section*.3}}}
\@writefile{brf}{\backcite{kiros2014multimodal,socher2014grounded,farhadi2010every}{{2}{2}{section*.3}}}
\@writefile{brf}{\backcite{farhadi2010every}{{2}{2}{section*.3}}}
\@writefile{brf}{\backcite{socher2014grounded,kiros2014multimodal,deepAlignment}{{2}{2}{section*.3}}}
\@writefile{toc}{\contentsline {paragraph}{Activity/Event Recognition:}{2}{section*.4}}
\@writefile{brf}{\backcite{kuehne2011hmdb, UCF101, niebles10_eccv, laptev08_cvpr, efros03_iccv, ryoo09_iccv}{{2}{2}{section*.4}}}
\@writefile{brf}{\backcite{THUMOS14, oneata2014lear, jainuniversity}{{2}{2}{section*.4}}}
\@writefile{brf}{\backcite{duchenne09_iccv, hoai11_cvpr, laptev07_iccv, bojanowski14_eccv, pirsiavash14_cvpr}{{2}{2}{section*.4}}}
\@writefile{brf}{\backcite{niebles10_eccv, yao10b_cvpr, jain13_cvpr,lan14_eccv, lan14_vs}{{2}{2}{section*.4}}}
\@writefile{brf}{\backcite{sun2014discover,das2013thousand,barbu2012video}{{2}{2}{section*.4}}}
\citation{cookingSemantics}
\citation{logicRecipe}
\citation{logicRecipe}
\citation{beetz}
\citation{cookie}
\citation{alignment}
\citation{photoshop}
\citation{cpmc}
\citation{keysegments}
\@writefile{toc}{\contentsline {paragraph}{Recipe Understanding:}{3}{section*.5}}
\@writefile{brf}{\backcite{cookingSemantics,logicRecipe}{{3}{2}{section*.5}}}
\@writefile{brf}{\backcite{logicRecipe}{{3}{2}{section*.5}}}
\@writefile{brf}{\backcite{beetz,cookie}{{3}{2}{section*.5}}}
\@writefile{brf}{\backcite{alignment}{{3}{2}{section*.5}}}
\@writefile{brf}{\backcite{photoshop}{{3}{2}{section*.5}}}
\@writefile{toc}{\contentsline {section}{\numberline {3}\hskip -1em.\nobreakspace  {}Overview}{3}{section.3}}
\newlabel{sec:overview}{{3}{3}{\hskip -1em.~Overview}{section.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}\hskip -1em.\nobreakspace  {}Forming the Multi-Modal Representation}{3}{section.4}}
\newlabel{atom}{{4}{3}{\hskip -1em.~Forming the Multi-Modal Representation}{section.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces We learn language and visual atoms in order to represent multi-modal information. Language atoms are frequent words and visual atoms are the clusters of object proposals.\relax }}{3}{figure.caption.6}}
\newlabel{fig:overview}{{2}{3}{We learn language and visual atoms in order to represent multi-modal information. Language atoms are frequent words and visual atoms are the clusters of object proposals.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {paragraph}{Learning Visual Atoms:}{3}{section*.7}}
\@writefile{brf}{\backcite{cpmc}{{3}{4}{section*.7}}}
\@writefile{brf}{\backcite{keysegments}{{3}{4}{section*.7}}}
\@writefile{toc}{\contentsline {paragraph}{Learning Language Atoms}{3}{section*.8}}
\citation{scgp}
\citation{scgp}
\citation{scgp_eigen}
\citation{alexnet}
\@writefile{toc}{\contentsline {paragraph}{Representing Frames with Atoms}{4}{section*.9}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {Visualization of the representation for a sample frame.} Three of the object proposals of the frame is included in the visual atoms and three of the words in the subtitle of the frame is included in the language atoms.\relax }}{4}{figure.caption.10}}
\newlabel{visFrame}{{3}{4}{\textbf {Visualization of the representation for a sample frame.} Three of the object proposals of the frame is included in the visual atoms and three of the words in the subtitle of the frame is included in the language atoms.\relax }{figure.caption.10}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}\hskip -1em.\nobreakspace  {}Joint Proposal Clustering over Videos}{4}{section.5}}
\newlabel{jointProp}{{5}{4}{\hskip -1em.~Joint Proposal Clustering over Videos}{section.5}{}}
\@writefile{toc}{\contentsline {paragraph}{Basic Graph Clustering:}{4}{section*.12}}
\@writefile{brf}{\backcite{scgp}{{4}{5}{section*.12}}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textbf  {Joint proposal clustering.} Here, we show the $1^{st}NN$ video graph and $2^{nd}NN$ region graph. Each region proposal is linked to its two NNs from the video it belongs and two NNs from the videos it is neighbour of. Black represents the proposals selected as part of the cluster and gray represents the ones which are not selected. Moreover, dashed lines are intra-video edges and solid ones are inter-video edges.\relax }}{4}{figure.caption.11}}
\newlabel{hierProposal}{{4}{4}{\textbf {Joint proposal clustering.} Here, we show the $1^{st}NN$ video graph and $2^{nd}NN$ region graph. Each region proposal is linked to its two NNs from the video it belongs and two NNs from the videos it is neighbour of. Black represents the proposals selected as part of the cluster and gray represents the ones which are not selected. Moreover, dashed lines are intra-video edges and solid ones are inter-video edges.\relax }{figure.caption.11}{}}
\newlabel{nonvec}{{1}{4}{Basic Graph Clustering:}{equation.5.1}{}}
\@writefile{brf}{\backcite{scgp,scgp_eigen}{{4}{5}{equation.5.1}}}
\@writefile{toc}{\contentsline {paragraph}{Joint Clustering:}{4}{section*.13}}
\@writefile{brf}{\backcite{alexnet}{{4}{5}{section*.13}}}
\citation{scgp}
\citation{scgp_eigen}
\citation{foxBPHMM}
\citation{foxBPHMM}
\citation{foxBPHMM}
\citation{ibp}
\citation{ibp}
\@writefile{brf}{\backcite{scgp,scgp_eigen}{{5}{5}{equation.5.2}}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Randomly selected images of four randomly selected clusters learned for \emph  {How to hard boil an egg?}\relax }}{5}{figure.caption.14}}
\newlabel{cvis}{{5}{5}{Randomly selected images of four randomly selected clusters learned for \emph {How to hard boil an egg?}\relax }{figure.caption.14}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}\hskip -1em.\nobreakspace  {}Unsupervised Activity Representation}{5}{subsection.5.1}}
\newlabel{basics}{{5.1}{5}{\hskip -1em.~Unsupervised Activity Representation}{subsection.5.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}Beta Process Hidden Markov Model}{5}{subsubsection.5.1.1}}
\newlabel{bphmm}{{5.1.1}{5}{Beta Process Hidden Markov Model}{subsubsection.5.1.1}{}}
\@writefile{brf}{\backcite{foxBPHMM}{{5}{5.1.1}{subsubsection.5.1.1}}}
\@writefile{brf}{\backcite{foxBPHMM}{{5}{5.1.1}{subsubsection.5.1.1}}}
\@writefile{brf}{\backcite{foxBPHMM}{{5}{5.1.1}{subsubsection.5.1.1}}}
\@writefile{brf}{\backcite{ibp}{{5}{5.1.1}{equation.5.4}}}
\@writefile{brf}{\backcite{ibp}{{5}{5.1.1}{equation.5.4}}}
\citation{foxBPHMM}
\citation{foxBPHMM}
\@writefile{brf}{\backcite{foxBPHMM}{{6}{5.1.1}{equation.5.4}}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.2}Gibbs sampling for BP-HMM}{6}{subsubsection.5.1.2}}
\@writefile{brf}{\backcite{foxBPHMM}{{6}{5.1.2}{subsubsection.5.1.2}}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces \textbf  {Graphical model for BP-HMM:} The left plate represent the set of activity steps and the right plate represent the set of videos. Intuitively, the left plate is for activity step discovery and right plate is for video parsing. Given a set of selected steps, the marginal model of the video becomes an Hidden Markov Model. \emph  {See Section\nobreakspace  {}\ref  {bphmm} for details.}\relax }}{6}{figure.caption.15}}
\newlabel{bphmmo}{{6}{6}{\textbf {Graphical model for BP-HMM:} The left plate represent the set of activity steps and the right plate represent the set of videos. Intuitively, the left plate is for activity step discovery and right plate is for video parsing. Given a set of selected steps, the marginal model of the video becomes an Hidden Markov Model. \emph {See Section~\ref {bphmm} for details.}\relax }{figure.caption.15}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}\hskip -1em.\nobreakspace  {}Experiments}{6}{section.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}\hskip -1em.\nobreakspace  {}Dataset}{6}{subsection.6.1}}
\newlabel{dataset:sec}{{6.1}{6}{\hskip -1em.~Dataset}{subsection.6.1}{}}
\citation{scgp}
\citation{languageModel}
\citation{fastLaptev}
\citation{rabiner}
\citation{potapov2014category}
\citation{potapov2014category}
\citation{trecc}
\citation{liao05}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.1.1}Outlier Detection}{7}{subsubsection.6.1.1}}
\newlabel{filter}{{6.1.1}{7}{Outlier Detection}{subsubsection.6.1.1}{}}
\@writefile{brf}{\backcite{scgp}{{7}{6.1.1}{subsubsection.6.1.1}}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces \textbf  {Sample videos which our algorithm discards as an outlier for various queries.} Detected outliers are a toy milkshake, a milkshake charm, a funny video about How to NOT make smoothie, an informative video about the danger of a fire, a cartoon about pancake, a neck-tie video erroneously labeled as bow-tie, a song including the phrase \emph  {How to tell if a gold is real?} and a lamb cooking mislabeled as \emph  {How to bake chicken?}\relax }}{7}{figure.caption.16}}
\newlabel{outliers}{{7}{7}{\textbf {Sample videos which our algorithm discards as an outlier for various queries.} Detected outliers are a toy milkshake, a milkshake charm, a funny video about How to NOT make smoothie, an informative video about the danger of a fire, a cartoon about pancake, a neck-tie video erroneously labeled as bow-tie, a song including the phrase \emph {How to tell if a gold is real?} and a lamb cooking mislabeled as \emph {How to bake chicken?}\relax }{figure.caption.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}\hskip -1em.\nobreakspace  {}Qualitative Results}{7}{subsection.6.2}}
\@writefile{brf}{\backcite{languageModel}{{7}{6.2}{subsection.6.2}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}\hskip -1em.\nobreakspace  {}Quantitative Results}{7}{subsection.6.3}}
\@writefile{brf}{\backcite{fastLaptev}{{7}{6.3}{figure.caption.18}}}
\@writefile{brf}{\backcite{rabiner}{{7}{6.3}{figure.caption.18}}}
\@writefile{brf}{\backcite{potapov2014category}{{7}{6.3}{figure.caption.18}}}
\@writefile{brf}{\backcite{potapov2014category}{{7}{6.3}{figure.caption.18}}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.1}Results}{7}{subsubsection.6.3.1}}
\@writefile{brf}{\backcite{trecc}{{7}{6.3.1}{subsubsection.6.3.1}}}
\@writefile{brf}{\backcite{liao05}{{7}{6.3.1}{subsubsection.6.3.1}}}
\newlabel{recipe:ommelette}{{8a}{8}{How to make an omelet?\relax }{figure.caption.17}{}}
\newlabel{sub@recipe:ommelette}{{a}{8}{How to make an omelet?\relax }{figure.caption.17}{}}
\newlabel{recipe:milkshake}{{8b}{8}{How to make a milkshake?\relax }{figure.caption.17}{}}
\newlabel{sub@recipe:milkshake}{{b}{8}{How to make a milkshake?\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Temporal segmentation of the videos by our method and ground truth segmentation. We also color code the discovered activity steps and visualize the key-frames and the automatically generated captions for some of them. \emph  {Best viewed in color.}\relax }}{8}{figure.caption.17}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces $IOU_{max}$ values for all recipes, for all competing algorithms.\relax }}{8}{figure.caption.18}}
\newlabel{mIOU}{{9}{8}{$IOU_{max}$ values for all recipes, for all competing algorithms.\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces $AP_{max}$ values for all recipes, for all competing algorithms.\relax }}{8}{figure.caption.18}}
\newlabel{mmAP}{{10}{8}{$AP_{max}$ values for all recipes, for all competing algorithms.\relax }{figure.caption.18}{}}
\@writefile{toc}{\contentsline {paragraph}{Accuracy of the temporal parsing.}{8}{section*.19}}
\citation{potapov2014category}
\citation{potapov2014category}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Average of $IOU_{cms}$ and $mAP_{cms}$ over recipes.\relax }}{9}{table.caption.20}}
\@writefile{brf}{\backcite{potapov2014category}{{9}{1}{table.caption.20}}}
\@writefile{brf}{\backcite{potapov2014category}{{9}{1}{table.caption.20}}}
\newlabel{averM}{{1}{9}{Average of $IOU_{cms}$ and $mAP_{cms}$ over recipes.\relax }{table.caption.20}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Semantic mean-average-precision $mAP_{sem}$.\relax }}{9}{table.caption.23}}
\@writefile{toc}{\contentsline {paragraph}{Coherency and accuracy of activity step discovery.}{9}{section*.21}}
\@writefile{toc}{\contentsline {paragraph}{Semantics of activity steps.}{9}{section*.22}}
\@writefile{toc}{\contentsline {paragraph}{The importance of each modality.}{9}{section*.24}}
\bibstyle{ieee}
\bibdata{recipeUnderstanding}
\bibcite{wikiHow}{1}
\bibcite{barbu2012video}{2}
\bibcite{matching}{3}
\bibcite{beetz}{4}
\bibcite{bojanowski14_eccv}{5}
\bibcite{cookie}{6}
\bibcite{cpmc}{7}
\bibcite{das2013thousand}{8}
\bibcite{duchenne09_iccv}{9}
\bibcite{efros03_iccv}{10}
\bibcite{farhadi2010every}{11}
\bibcite{fidler2013sentence}{12}
\bibcite{foxBPHMM}{13}
\bibcite{photoshop}{14}
\bibcite{ibp}{15}
\bibcite{gupta2009understanding}{16}
\bibcite{createSum}{17}
\bibcite{hoai11_cvpr}{18}
\bibcite{beyondSearch}{19}
\bibcite{jain13_cvpr}{20}
\bibcite{jainuniversity}{21}
\bibcite{THUMOS14}{22}
\bibcite{deepAlignment}{23}
\bibcite{khosla2013large}{24}
\bibcite{kim2014joint}{25}
\bibcite{storyGraph}{26}
\bibcite{kiros2014multimodal}{27}
\bibcite{kong2014you}{28}
\bibcite{kuehne2011hmdb}{29}
\bibcite{lan14_vs}{30}
\bibcite{lan14_eccv}{31}
\bibcite{laptev08_cvpr}{32}
\bibcite{laptev07_iccv}{33}
\bibcite{lee2012discovering}{34}
\bibcite{keysegments}{35}
\bibcite{lu2013story}{36}
\bibcite{alignment}{37}
\bibcite{cookingSemantics}{38}
\bibcite{flowGraph}{39}
\bibcite{motwani2012improving}{40}
\bibcite{niebles10_eccv}{41}
\bibcite{scgp}{42}
\bibcite{oneata2014lear}{43}
\bibcite{ordonez2011im2text}{44}
\bibcite{scgp_eigen}{45}
\bibcite{pirsiavash14_cvpr}{46}
\bibcite{potapov2014category}{47}
\bibcite{rabiner}{48}
\bibcite{rui2000automatically}{49}
\bibcite{ryoo09_iccv}{50}
\bibcite{connecting}{51}
\bibcite{socher2014grounded}{52}
\bibcite{UCF101}{53}
\bibcite{sun2014discover}{54}
\bibcite{logicRecipe}{55}
\bibcite{vidAbstraction}{56}
\bibcite{yao10b_cvpr}{57}
\bibcite{yu2013grounded}{58}
\bibcite{zitnick2013bringing}{59}
\bibcite{zitnick2013learning}{60}
